{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "CV2021_ps4.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/santomon/cv2021_4/blob/master/CV2021_ps4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6864e6b"
      },
      "source": [
        "# Computer Vision\n",
        "\n",
        "---\n",
        "\n",
        "**Goethe University Frankfurt am Main**\n",
        "\n",
        "Winter Semester 2021/22\n",
        "\n",
        "<br>\n",
        "\n",
        "## *Problem Set 4*\n",
        "\n",
        "---\n",
        "\n",
        "**Points:** 200<br>\n",
        "**Due:** 21. December 2021, 10am<br>\n",
        "**Contact:** Matthias Fulde ([fulde@cvai.cs.uni-frankfurt.de](mailto:fulde@cvai.cs.uni-frankfurt.de))\n",
        "\n",
        "---\n",
        "\n",
        "**Name:** *Please write your name here.*\n",
        "\n",
        "<br>"
      ],
      "id": "a6864e6b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad9fd2ad"
      },
      "source": [
        "## Notes\n",
        "\n",
        "---\n",
        "\n",
        "Before you submit your solutions, please make sure that you\n",
        "\n",
        "- wrote your name in the **Name** field above.\n",
        "- wrote your code only into the designated areas delimited by `START OF YOUR CODE` and `END OF YOUR CODE` comments.\n",
        "- removed all test code and test outputs you added during development.\n",
        "- executed all code cells that generate output without error.\n",
        "\n",
        "Your submission should be an archive that contains all files and folders provided with this notebook, but **not** the dataset.\n",
        "\n",
        "<br>\n"
      ],
      "id": "ad9fd2ad"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeca6f81"
      },
      "source": [
        "## Table of Contents\n",
        "\n",
        "---\n",
        "\n",
        "- [1 Layers](#1-Layers-(160-Points))\n",
        "  - [1.1 Vectorization Layer](#1.1-Vectorization-Layer-(5-Points))\n",
        "  - [1.2 Linear Layer](#1.2-Linear-Layer-(35-Points))\n",
        "  - [1.3 Dropout Layer](#1.3-Dropout-Layer-(25-Points))\n",
        "  - [1.4 Convolutional Layer](#1.4-Convolutional-Layer-(50-Points))\n",
        "  - [1.5 Max Pooling Layer](#1.5-Max-Pooling-Layer-(35-Points))\n",
        "  - [1.6 ReLU Activation Layer](#1.6-ReLU-Activation-Layer-(10-Points))\n",
        "- [2 Loss Function](#2-Loss-Function-(20-Points))\n",
        "  - [2.1 Cross-entropy Loss](#2.1-Cross-entropy-Loss-(20-Points))\n",
        "- [3 Optimization](#3-Optimization-(10-Points))\n",
        "  - [3.1 SGD with Momentum](#3.1-SGD-with-Momentum-(10-Points))\n",
        "- [4 Deep Neural Network](#4-Deep-Neural-Network-(10-Points))\n",
        "  - [4.1 Training](#4.1-Training-(10-Points))\n",
        "\n",
        "<br>"
      ],
      "id": "eeca6f81"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe8c743f"
      },
      "source": [
        "## Setup\n",
        "\n",
        "---\n",
        "\n",
        "In this problem set we **only** work with NumPy and Matplotlib.\n",
        "\n",
        "We set the Matplotlib backend to inline in order to display images directly in the notebook. In addition, we load all modules containing the classes we want to implement in this problem set and enable autoreloading, so that we don't have to reload the modules manually when we've made an edit.\n",
        "\n",
        "For this problem set, we recommend to use at least Python 3.7."
      ],
      "id": "fe8c743f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fde2b284"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "from modules import *\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "id": "fde2b284",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb8dc44f"
      },
      "source": [
        "## Definitions\n",
        "\n",
        "---\n",
        "\n",
        "We define an `error` function to measure the relative difference between two outcomes."
      ],
      "id": "cb8dc44f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97129c14"
      },
      "source": [
        "def error(x, y):\n",
        "    \"\"\"\n",
        "    Calculate the sum of the relative differences.\n",
        "    The absolute differences are scaled with the sum of the absolute values.\n",
        "    \"\"\"\n",
        "    x = x.astype(np.float32)\n",
        "    y = y.astype(np.float32)\n",
        "\n",
        "    return np.sum(abs(x - y) / np.maximum(1e-8, abs(x) + abs(y)))"
      ],
      "id": "97129c14",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "851329c7"
      },
      "source": [
        "## Exercises\n",
        "\n",
        "---\n",
        "\n",
        "### 1 Layers (160 Points)\n",
        "\n",
        "---\n",
        "\n",
        "Our goal in this problem set is to implement a deep neural network composed of different kinds of layers.\n",
        "\n",
        "The first part of the network should be a CNN, composed of convolutional and max pooling layers, and the second part should be a FCN with linear layers, for which we also want to implement dropout for regularization. We're going to use standard rectified linear units as nonlinear activation functions for both the convolutional and the linear layers.\n",
        "\n",
        "In this exercise we implement the different layers so that we can easily plug them into the model later.\n",
        "\n",
        "<br>\n",
        "\n",
        "### 1.1 Vectorization Layer (5 Points)\n",
        "\n",
        "---\n",
        "\n",
        "Since we want to use linear layers, we need a function to convert tensor input into vectors. Moreover, since we want to implement a small FCN on top of a CNN, we also need a function to reverse this operation, in order to pass the gradient of the loss with the correct shape to the last convolutional layer.\n",
        "\n",
        "##### Task\n",
        "\n",
        "Complete the definition of the `Vector` class in the `layers/vector.py` file. Your implementation should be fully vectorized, so no loops are allowed.\n",
        "\n",
        "In the `forward` method, store the shape of the input to be used in the backward pass. Convert the inputs into vectors such that the result is a matrix where each row is one input. Store the result in the `outputs` variable that is returned from the method.\n",
        "\n",
        "In the `backward` method, apply the revesed operation and restore the original shape for the gradient of the loss that the method receives from the following layer. Store the result in the `in_grad` variable that is returned from the method.\n",
        "\n",
        "##### Results\n",
        "\n",
        "To test your implementation, run the following code cells."
      ],
      "id": "851329c7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a8cb1cd"
      },
      "source": [
        "# Set shape for test inputs and outputs.\n",
        "inputs_shape  = (2, 3, 4, 5)\n",
        "outputs_shape = (2, 60)\n",
        "\n",
        "# Create test inputs.\n",
        "inputs = np.zeros(inputs_shape)\n",
        "\n",
        "# Create upstream gradient.\n",
        "out_grad = np.zeros(outputs_shape)\n",
        "\n",
        "# Create layer.\n",
        "layer = Vector()"
      ],
      "id": "2a8cb1cd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78f3eeb9"
      },
      "source": [
        "You can test your `forward` method with the following code."
      ],
      "id": "78f3eeb9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cce609dc"
      },
      "source": [
        "# Compute forward pass.\n",
        "outputs = layer.forward(inputs)\n",
        "\n",
        "# Compare shapes\n",
        "print(f'Forward method correct: {outputs.shape == outputs_shape}')"
      ],
      "id": "cce609dc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77641861"
      },
      "source": [
        "You can test your `backward` method with the following code."
      ],
      "id": "77641861"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "488b39ca"
      },
      "source": [
        "# Compute forward pass.\n",
        "outputs = layer.forward(inputs)\n",
        "\n",
        "# Compute backward pass.\n",
        "in_grad = layer.backward(out_grad)\n",
        "\n",
        "# Compare shapes.\n",
        "print(f'Backward method correct: {in_grad.shape == inputs_shape}')"
      ],
      "id": "488b39ca",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e6bb61f"
      },
      "source": [
        "### 1.2 Linear Layer (35 Points)\n",
        "\n",
        "---\n",
        "\n",
        "The fully-connected network is composed of linear or affine layers that map input vectors with shape $(\\text{num_samples},\\text{in_features})$ to output vectors with shape $(\\text{num_samples},\\text{out_features})$, where the difference between a linear and an affine map is whether a bias is added or not. In this exercise we want to implement such a layer as described below.\n",
        "\n",
        "##### Task\n",
        "\n",
        "Complete the definition of `Linear` class in the `layers/linear.py` file. Your implementation should be fully vectorized, that is, no loops are allowed.\n",
        "\n",
        "In the `__init__` method, store a flag indicating if a linear or affine transformation should be used.<br>\n",
        "Initialize the parameters with random values as follows:<br>\n",
        "\n",
        "- The parameters are stored in a dictionary already created in the base class. Save the weights and, conditionally, the bias, as in\n",
        "  ```python\n",
        "  self.param['weights'] = ...\n",
        "  self.param['bias'] = ...\n",
        "  ```\n",
        "- The weights should have the shape $(\\text{in_features},\\text{out_features})$ and the bias should have the shape $(\\text{out_features})$, where $\\text{in_features}$ is the length of the input vectors and $\\text{out_features}$ is the length of the output vectors.\n",
        "- Initialize each parameter value from $\\text{Uniform}(-\\sqrt{k},\\sqrt{k})$ where $k = \\frac{1}{\\text{in_features}}$.\n",
        "   \n",
        "<br>\n",
        "\n",
        "In the `forward` method, store the received inputs for gradient computation in the backward pass. Depending on the stored flag, apply a linear or affine transformation to the inputs and store the result in the `outputs` variable that is returned from the method.\n",
        "   \n",
        "<br>\n",
        "\n",
        "In the `backward` method, compute the gradient of the loss with respect to the parameters and inputs. The method receives the gradient of the loss with respect to the layer output.\n",
        "   \n",
        "- The gradient of the loss with respect to the weights and, if required, the bias, is stored in a dictionary inherited from the base class. Save the parameters using the same keys that were used for the parameter dictionary, as in\n",
        "  ```python\n",
        "  self.grad['weights'] = ...\n",
        "  self.grad['bias'] = ...\n",
        "  ```\n",
        "- Store the gradient of the loss with respect to the layer inputs in the `in_grad` variable that is returned from the method.\n",
        "\n",
        "##### Results\n",
        "\n",
        "To test your implementation, run the following code cells."
      ],
      "id": "0e6bb61f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7d83e195"
      },
      "source": [
        "# Set number of samples.\n",
        "num_inputs = 2\n",
        "\n",
        "# Set input and output dimensions.\n",
        "input_dim = 5\n",
        "output_dim = 3\n",
        "\n",
        "# Create inputs.\n",
        "inputs = np.linspace(-0.1, 0.5, num=num_inputs * input_dim)\n",
        "inputs = np.reshape(inputs, (num_inputs, input_dim))\n",
        "\n",
        "# Create weights.\n",
        "weights = np.linspace(-0.2, 0.3, num=output_dim * input_dim)\n",
        "weights = np.reshape(weights, (input_dim, output_dim))\n",
        "\n",
        "# Create bias.\n",
        "bias = np.linspace(-0.3, 0.1, num=output_dim)\n",
        "\n",
        "# Create upstream gradient.\n",
        "out_grad = np.linspace(-0.2, 0.4, num=num_inputs * output_dim)\n",
        "out_grad = np.reshape(out_grad, (num_inputs, output_dim))\n",
        "\n",
        "# Create affine layer.\n",
        "layer = Linear(input_dim, output_dim)\n",
        "\n",
        "# Reset parameters.\n",
        "layer.param['weights'] = weights\n",
        "layer.param['bias'] = bias"
      ],
      "id": "7d83e195",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d44c4d66"
      },
      "source": [
        "You can test your `forward` method with the following code."
      ],
      "id": "d44c4d66"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91656517"
      },
      "source": [
        "# Compute forward pass.\n",
        "outputs = layer.forward(inputs)\n",
        "\n",
        "correct_outputs = np.array([\n",
        "    [-0.22619048, -0.0202381,   0.18571429],\n",
        "    [-0.20238095,  0.06309524,  0.32857143]\n",
        "])\n",
        "\n",
        "# Compare outputs.\n",
        "print(f'Forward method correct: {error(outputs, correct_outputs) < 1e-5}')"
      ],
      "id": "91656517",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0121f61f"
      },
      "source": [
        "You can test your `backward` method with the following code."
      ],
      "id": "0121f61f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1e47c4ce"
      },
      "source": [
        "# Compute forward pass.\n",
        "outputs = layer.forward(inputs)\n",
        "\n",
        "# Compute backward pass.\n",
        "in_grad = layer.backward(out_grad)\n",
        "\n",
        "# Get gradients for parameters.\n",
        "weights_grad = layer.grad['weights']\n",
        "bias_grad = layer.grad['bias']\n",
        "\n",
        "# Compare gradients.\n",
        "results = [\n",
        "\n",
        "    error(in_grad, np.array([\n",
        "        [ 0.048,       0.02228571, -0.00342857, -0.02914286, -0.05485714],\n",
        "        [-0.12942857, -0.03942857,  0.05057143,  0.14057143,  0.23057143]\n",
        "    ])),\n",
        "    \n",
        "    error(weights_grad, np.array([\n",
        "        [0.05733333, 0.07333333, 0.08933333],\n",
        "        [0.05466667, 0.08666667, 0.11866667],\n",
        "        [0.052,      0.1,        0.148     ],\n",
        "        [0.04933333, 0.11333333, 0.17733333],\n",
        "        [0.04666667, 0.12666667, 0.20666667]\n",
        "    ])),\n",
        "    \n",
        "    error(bias_grad, np.array([-0.04, 0.2, 0.44]))\n",
        "]\n",
        "\n",
        "# Show results.\n",
        "print(f'Backward method correct: {all(e < 1e-5 for e in results)}')"
      ],
      "id": "1e47c4ce",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d77712c"
      },
      "source": [
        "### 1.3 Dropout Layer (25 Points)\n",
        "\n",
        "---\n",
        "\n",
        "In the lecture, dropout was introduced as a regularization method. Applied to the activations of a linear layer, a random binary mask is created such that each activation unit is either kept with probability $p$, or set to zero with probability $1 - p$. Thus, in every training iteration, a different subset of neurons is learned, forcing the network to learn redundant feature representations.\n",
        "\n",
        "##### Task\n",
        "\n",
        "Complete the definition of the `Dropout` class in the `layers/dropout.py` file. Your implementation should be fully vectorized, so no loops are allowed.\n",
        "\n",
        "<br>\n",
        "\n",
        "In the `forward` method, apply dropout when the `self.training` attribute is true. Otherwise just pass the input. For the implementation of dropout\n",
        "\n",
        "- create a random binary mask with the same shape as the input, such that each entry is $1$ with probability $p$ and $0$ with probability $1 - p$.\n",
        "- scale the mask with $\\frac{1}{p}$ so that we don't have to scale the activations during inference, where all neurons should be kept active.\n",
        "- multiply each input feature with the corresponding entry in the mask.\n",
        "- store the mask for gradient computation in the backward pass.\n",
        "\n",
        "Store the transformed activations in the `outputs` variable that is returned by the method.\n",
        "\n",
        "<br>\n",
        "\n",
        "In the `backward` method, compute the gradient of the loss with respect to the input. The local gradient of the dropout layer with respect to the input is just the random mask created in the forward pass. This must be combined with the gradient received from the next layer. Store the result in the `in_grad` variable that is returned by the method.\n",
        "\n",
        "##### Results\n",
        "\n",
        "To test your implementation, run the following code cells."
      ],
      "id": "7d77712c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e11eb99"
      },
      "source": [
        "# Create test inputs.\n",
        "inputs = np.random.randn(1000, 1000) + 10\n",
        "\n",
        "# Create upstream gradient.\n",
        "out_grad = np.random.randn(1000, 1000) + 10\n",
        "\n",
        "# Create candidate probabilities.\n",
        "probs = [0.3, 0.5, 0.7]"
      ],
      "id": "9e11eb99",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "218544a8"
      },
      "source": [
        "You can test your `forward` method with the following code."
      ],
      "id": "218544a8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c854eb96"
      },
      "source": [
        "# Create list for results.\n",
        "results = []\n",
        "\n",
        "# Compute forward passes.\n",
        "for p in probs:\n",
        "    layer = Dropout(p)\n",
        "\n",
        "    # Get fraction of dropped neurons during training.\n",
        "    outputs = layer.forward(inputs)\n",
        "    train_result = np.around(np.mean(outputs == 0), decimals=1) + p\n",
        "\n",
        "    layer.training = False\n",
        "\n",
        "    # Get fraction of dropped neurons during testing.\n",
        "    outputs = layer.forward(inputs)\n",
        "    test_result = np.around(np.mean(outputs == 0), decimals=1)\n",
        "    \n",
        "    # Store results.\n",
        "    results.append((train_result, test_result))\n",
        "\n",
        "# Compare results.\n",
        "print(f'Forward method correct: {all(res == (1, 0) for res in results)}')"
      ],
      "id": "c854eb96",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b92d39e"
      },
      "source": [
        "You can test your `backward` method with the following code."
      ],
      "id": "0b92d39e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9b4d225e"
      },
      "source": [
        "# Create layer.\n",
        "layer = Dropout(0.5)\n",
        "\n",
        "# Compute forward pass.\n",
        "outputs = layer.forward(inputs)\n",
        "\n",
        "# Compute backward pass.\n",
        "in_grad = layer.backward(out_grad)\n",
        "\n",
        "# Compare results.\n",
        "print(f'Backward method correct: {np.array_equal(np.where(outputs == 0), np.where(in_grad == 0))}')"
      ],
      "id": "9b4d225e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "279d4fef"
      },
      "source": [
        "### 1.4 Convolutional Layer (50 Points)\n",
        "\n",
        "---\n",
        "\n",
        "For the CNN part of our model we need convolutional layers. For inputs with shape $(\\text{num_samples},\\text{in_channels},\\text{in_height},\\text{in_width})$ the layer should generate output with shape $(\\text{num_samples},\\text{out_channels},\\text{out_height},\\text{out_width})$, with the spacial dimensions of the output depending on the size of the input, the kernel size and the padding and stride applied in the convolution.\n",
        "\n",
        "##### Task\n",
        "\n",
        "Complete the definition of `Conv2D` class in the `layers/conv.py` file. Your implementation should be at least partly vectorized, that is, you're allowed to use at most *four* loops each for the forward and backward pass.\n",
        "\n",
        "In the `__init__` method, store the given values for padding and stride, and a flag indicating if a bias should be used or not.<br>\n",
        "Initialize the parameters with random values as follows:<br>\n",
        "\n",
        "- The parameters are stored in a dictionary already created in the base class. Save the weights and, conditionally, the bias, as in\n",
        "  ```python\n",
        "  self.param['weights'] = ...\n",
        "  self.param['bias'] = ...\n",
        "  ```\n",
        "- The weights should have the shape $(\\text{out_channels},\\text{in_channels},\\text{kernel_size},\\text{kernel_size})$ and the bias should have the shape $(\\text{out_channels})$, thus you can assume that the filters are always square.\n",
        "- Initialize each parameter value from $\\text{Uniform}(-\\sqrt{k},\\sqrt{k})$ where $k = \\frac{1}{\\text{in_channels} \\:\\cdot\\: \\text{kernel_size}^2}$.\n",
        "   \n",
        "<br>\n",
        "\n",
        "In the `forward` method, store the received inputs for gradient computation in the backward pass. Convolve the inputs with the filters using the stored values for padding and stride and conditionally add the bias, then store the result in the `outputs` variable that is returned from the method.\n",
        "   \n",
        "<br>\n",
        "\n",
        "In the `backward` method, compute the gradient of the loss with respect to the parameters and inputs. The method receives the gradient of the loss with respect to the layer output.\n",
        "   \n",
        "- The gradient of the loss with respect to the weights and, if required, the bias, is stored in a dictionary inherited from the base class. Save the parameters using the same keys that were used for the parameter dictionary, as in\n",
        "  ```python\n",
        "  self.grad['weights'] = ...\n",
        "  self.grad['bias'] = ...\n",
        "  ```\n",
        "- Store the gradient of the loss with respect to the layer inputs in the `in_grad` variable that is returned from the method.\n",
        "\n",
        "##### Results\n",
        "\n",
        "To test your implementation, run the following code cells."
      ],
      "id": "279d4fef"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ecdb1dc"
      },
      "source": [
        "# Set dimensions for inputs, outputs and weights.\n",
        "inputs_shape  = (2, 2, 3, 3)\n",
        "outputs_shape = (2, 2, 2, 2)\n",
        "weights_shape = (2, 2, 3, 3)\n",
        "\n",
        "# Create inputs.\n",
        "inputs = np.linspace(-0.1, 0.5, num=np.prod(inputs_shape))\n",
        "inputs = np.reshape(inputs, inputs_shape)\n",
        "\n",
        "# Create weights.\n",
        "weights = np.linspace(-0.2, 0.3, num=np.prod(weights_shape))\n",
        "weights = np.reshape(weights, weights_shape)\n",
        "\n",
        "# Create bias.\n",
        "bias = np.linspace(-0.1, 0.2, num=3)\n",
        "\n",
        "# Create upstream gradient.\n",
        "out_grad = np.linspace(-0.2, 0.3, num=np.prod(outputs_shape))\n",
        "out_grad = np.reshape(out_grad, outputs_shape)\n",
        "\n",
        "# Create conv layer.\n",
        "layer = Conv2D(2, 2, kernel_size=3, padding=1, stride=2)\n",
        "\n",
        "# Reset parameters.\n",
        "layer.param['weights'] = weights\n",
        "layer.param['bias'] = bias"
      ],
      "id": "7ecdb1dc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cc8bc77"
      },
      "source": [
        "You can test your `forward` method with the following code."
      ],
      "id": "0cc8bc77"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ab9e2f9"
      },
      "source": [
        "# Compute forward pass.\n",
        "outputs = layer.forward(inputs)\n",
        "\n",
        "correct_outputs = np.array([\n",
        "    [[[-0.06,       -0.07012245],\n",
        "      [-0.10212245, -0.124     ]],\n",
        "     [[ 0.1135102,   0.13865306],\n",
        "      [ 0.17718367,  0.19057143]]],\n",
        "    [[[-0.18342857, -0.22881633],\n",
        "      [-0.33134694, -0.3884898 ]],\n",
        "     [[ 0.62485714,  0.61473469],\n",
        "      [ 0.58273469,  0.56085714]]]\n",
        "])\n",
        "\n",
        "# Compare outputs\n",
        "print(f'Forward method correct: {error(outputs, correct_outputs) < 1e-5}')"
      ],
      "id": "7ab9e2f9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "362fc609"
      },
      "source": [
        "You can test your `backward` method with the following code."
      ],
      "id": "362fc609"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38ca6d84"
      },
      "source": [
        "# Compute forward pass.\n",
        "outputs = layer.forward(inputs)\n",
        "\n",
        "# Compute backward pass.\n",
        "in_grad = layer.backward(out_grad)\n",
        "\n",
        "# Get gradients for parameters.\n",
        "weights_grad = layer.grad['weights']\n",
        "bias_grad = layer.grad['bias']\n",
        "\n",
        "# Compare gradients.\n",
        "results = [\n",
        "\n",
        "    error(in_grad, np.array([\n",
        "        [[[ 0.02095238,  0.04,        0.02      ],\n",
        "          [ 0.03428571,  0.0647619,   0.03238095],\n",
        "          [ 0.01904762,  0.03619048,  0.01809524]],\n",
        "         [[-0.01333333, -0.02,       -0.00571429],\n",
        "          [-0.01714286, -0.02095238, -0.00190476],\n",
        "          [ 0.00190476,  0.01047619,  0.00952381]]],\n",
        "        [[[ 0.01333333,  0.0247619,   0.01238095],\n",
        "          [ 0.01904762,  0.03428571,  0.01714286],\n",
        "          [ 0.01142857,  0.02095238,  0.01047619]],\n",
        "         [[ 0.04761905,  0.10190476,  0.0552381 ],\n",
        "          [ 0.1047619,   0.22285714,  0.12      ],\n",
        "          [ 0.06285714,  0.13238095,  0.07047619]]]\n",
        "    ])),\n",
        "\n",
        "    error(weights_grad, np.array([\n",
        "        [[[0.04933333, 0.09161905, 0.04114286],\n",
        "          [0.08914286, 0.16419048, 0.0727619 ],\n",
        "          [0.03295238, 0.05885714, 0.0247619 ]],\n",
        "         [[0.05961905, 0.10190476, 0.04114286],\n",
        "          [0.08914286, 0.14361905, 0.05219048],\n",
        "          [0.02266667, 0.028,      0.00419048]]],\n",
        "        [[[0.08209524, 0.15714286, 0.07390476],\n",
        "          [0.15466667, 0.2952381,  0.13828571],\n",
        "          [0.06571429, 0.12438095, 0.05752381]],\n",
        "         [[0.13352381, 0.24971429, 0.11504762],\n",
        "          [0.23695238, 0.4392381,  0.2       ],\n",
        "          [0.09657143, 0.17580952, 0.07809524]]]\n",
        "    ])),\n",
        "\n",
        "    error(bias_grad, np.array([-0.13333333, 0.93333333, 0.]))\n",
        "\n",
        "]\n",
        "\n",
        "# Show results.\n",
        "print(f'Backward method correct: {all(e < 1e-5 for e in results)}')"
      ],
      "id": "38ca6d84",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e56121da"
      },
      "source": [
        "### 1.5 Max Pooling Layer (35 Points)\n",
        "\n",
        "---\n",
        "\n",
        "In order to reduce the size of the feature maps in the CNN, we want to use max pooling. For each channel separately, the input is filtered such that only the maximum activation in each filter position is selected.\n",
        "\n",
        "##### Task\n",
        "\n",
        "Complete the definition of `MaxPool` class in the `layers/pool.py` file. Your implementation should be at least partly vectorized, that is, you're allowed to use at most *four* loops each for the forward and backward pass.\n",
        "\n",
        "In the `__init__` method, store the given values for kernel size and stride. If no stride is given, set the value to the kernel size, so that non-overlapping windows are used for pooling.\n",
        "   \n",
        "<br>\n",
        "\n",
        "In the `forward` method, store the received inputs for gradient computation in the backward pass. Apply max pooling to the input using the given kernel size and stride, then store the result in the `outputs` variable that is returned from the method.\n",
        "   \n",
        "<br>\n",
        "\n",
        "In the `backward` method, compute the gradient of the loss with respect to the inputs. The method receives the gradient of the loss with respect to the layer output. Store the gradient of the loss with respect to the layer inputs in the `in_grad` variable that is returned from the method.\n",
        "\n",
        "##### Results\n",
        "\n",
        "To test your implementation, run the following code cells."
      ],
      "id": "e56121da"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bbfe408"
      },
      "source": [
        "# Define shapes for input and output.\n",
        "inputs_shape  = (2, 2, 4, 4)\n",
        "outputs_shape = (2, 2, 2, 2)\n",
        "\n",
        "# Create inputs.\n",
        "inputs = np.linspace(-0.3, 0.5, num=np.prod(inputs_shape))\n",
        "inputs = np.reshape(inputs, inputs_shape)\n",
        "\n",
        "# Create upstream gradient.\n",
        "out_grad = np.linspace(-0.2, 0.3, num=np.prod(outputs_shape))\n",
        "out_grad = np.reshape(out_grad, outputs_shape)\n",
        "\n",
        "# Create layer.\n",
        "layer = MaxPool(2)"
      ],
      "id": "6bbfe408",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c9345c6"
      },
      "source": [
        "You can test your `forward` method with the following code."
      ],
      "id": "7c9345c6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "558b2cd1"
      },
      "source": [
        "# Compute forward pass.\n",
        "outputs = layer.forward(inputs)\n",
        "\n",
        "correct_outputs = np.array([\n",
        "    [[[-0.23650794, -0.21111111],\n",
        "      [-0.13492063, -0.10952381]],\n",
        "     [[-0.03333333, -0.00793651],\n",
        "      [ 0.06825397,  0.09365079]]],\n",
        "    [[[ 0.16984127,  0.1952381 ],\n",
        "      [ 0.27142857,  0.2968254 ]],\n",
        "     [[ 0.37301587,  0.3984127 ],\n",
        "      [ 0.47460317,  0.5       ]]]\n",
        "])\n",
        "\n",
        "# Compare outputs.\n",
        "print(f'Forward method correct: {error(outputs, correct_outputs) < 1e-5}')"
      ],
      "id": "558b2cd1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b14e79e1"
      },
      "source": [
        "You can test your `backward` method with the following code."
      ],
      "id": "b14e79e1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76b294fb"
      },
      "source": [
        "# Compute forward pass.\n",
        "outputs = layer.forward(inputs)\n",
        "\n",
        "# Compute backward pass.\n",
        "in_grad = layer.backward(out_grad)\n",
        "\n",
        "correct_in_grad = np.array([\n",
        "    [[[0.,  0.,         0.,  0.        ],\n",
        "      [0., -0.2,        0., -0.16666667],\n",
        "      [0.,  0.,         0.,  0.        ],\n",
        "      [0., -0.13333333, 0., -0.1       ]],\n",
        "     [[0.,  0.,         0.,  0.        ],\n",
        "      [0., -0.06666667, 0., -0.03333333],\n",
        "      [0.,  0.,         0.,  0.        ],\n",
        "      [0.,  0.,         0.,  0.03333333]]],\n",
        "    [[[0.,  0.,         0.,  0.        ],\n",
        "      [0.,  0.06666667, 0.,  0.1       ],\n",
        "      [0.,  0.,         0.,  0.        ],\n",
        "      [0.,  0.13333333, 0.,  0.16666667]],\n",
        "     [[0.,  0.,         0.,  0.        ],\n",
        "      [0.,  0.2,        0.,  0.23333333],\n",
        "      [0.,  0.,         0.,  0.        ],\n",
        "      [0.,  0.26666667, 0.,  0.3       ]]]\n",
        "])\n",
        "\n",
        "# Compare results.\n",
        "print(f'Backward method correct: {error(in_grad, correct_in_grad) < 1e-5}')"
      ],
      "id": "76b294fb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad31a8a1"
      },
      "source": [
        "### 1.6 ReLU Activation Layer (10 Points)\n",
        "\n",
        "---\n",
        "\n",
        "For both the convolutional and fully-connected layers we want to use standard rectified linear units as activation function. The function is applied elementwise such that all positive values are left unchanged, while all negative values are set to zero.\n",
        "\n",
        "##### Task\n",
        "\n",
        "Complete the definition of the `ReLU` class in the `layers/relu.py` file. Your implementation should be fully vectorized, that is, no loops are allowed.\n",
        "\n",
        "In the `forward` method, store the received inputs for gradient computation in the backward pass. Apply the ReLU activation function to the input componentwise and store the result in the `outputs` variable that is returned from the method.\n",
        "\n",
        "In the `backward` method, compute the gradient of the loss with respect to the inputs. The method receives the gradient of the loss with respect to the layer output. Store the gradient of the loss with respect to the layer inputs in the `in_grad` variable that is returned from the method.\n",
        "\n",
        "##### Results\n",
        "\n",
        "To test your implementation, run the following code cells."
      ],
      "id": "ad31a8a1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a6e2f9b"
      },
      "source": [
        "# Create inputs.\n",
        "inputs = np.array([\n",
        "    [[[-0.01640701, -0.57770197],\n",
        "      [ 0.37487488, -0.77260795]],\n",
        "     [[ 0.31117251,  1.28778772],\n",
        "      [-0.9739217,  -0.15076198]],\n",
        "     [[ 0.67561644, -0.75053469],\n",
        "      [-0.68843359,  1.13142207]]],\n",
        "    [[[-0.48168604,  0.37160233],\n",
        "      [ 0.41082495, -0.40195236]],\n",
        "     [[-1.4123589,   2.57995339],\n",
        "      [-0.20205541,  0.5211701 ]],\n",
        "     [[ 0.36500362, -1.6598223 ],\n",
        "      [ 3.39411579, -1.46596421]]]\n",
        "])\n",
        "\n",
        "# Create upstream gradient.\n",
        "out_grad = np.linspace(-0.3, 0.4, num=np.prod(inputs.shape))\n",
        "out_grad = np.reshape(out_grad, inputs.shape)\n",
        "\n",
        "# Create layer.\n",
        "layer = ReLU()"
      ],
      "id": "3a6e2f9b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "614e5543"
      },
      "source": [
        "You can test your `forward` method with the following code."
      ],
      "id": "614e5543"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7fc1305"
      },
      "source": [
        "# Compute forward pass.\n",
        "outputs = layer.forward(inputs)\n",
        "\n",
        "correct_outputs = np.array([\n",
        "    [[[0.,         0.        ],\n",
        "      [0.37487488, 0.        ]],\n",
        "     [[0.31117251, 1.28778772],\n",
        "      [0.,         0.        ]],\n",
        "     [[0.67561644, 0.        ],\n",
        "      [0.,         1.13142207]]],\n",
        "    [[[0.,         0.37160233],\n",
        "      [0.41082495, 0.        ]],\n",
        "     [[0.,         2.57995339],\n",
        "      [0.,         0.5211701 ]],\n",
        "     [[0.36500362, 0.        ],\n",
        "      [3.39411579, 0.        ]]]\n",
        "])\n",
        "\n",
        "# Compare outputs.\n",
        "print(f'Forward method correct: {error(outputs, correct_outputs) < 1e-5}')"
      ],
      "id": "b7fc1305",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "278dcc36"
      },
      "source": [
        "You can test your `backward` method with the following code."
      ],
      "id": "278dcc36"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c94f0390"
      },
      "source": [
        "# Compute forward pass.\n",
        "outputs = layer.forward(inputs)\n",
        "\n",
        "# Compute backward pass.\n",
        "in_grad = layer.backward(out_grad)\n",
        "\n",
        "correct_in_grad = np.array([\n",
        "    [[[-0.,         -0.        ],\n",
        "      [-0.23913043, -0.        ]],\n",
        "     [[-0.17826087, -0.14782609],\n",
        "      [-0.,         -0.        ]],\n",
        "     [[-0.05652174, -0.        ],\n",
        "      [ 0.,          0.03478261]]],\n",
        "    [[[ 0.,          0.09565217],\n",
        "      [ 0.12608696,  0.        ]],\n",
        "     [[ 0.,          0.2173913 ],\n",
        "      [ 0.,          0.27826087]],\n",
        "     [[ 0.30869565,  0.        ],\n",
        "      [ 0.36956522,  0.        ]]]\n",
        "])\n",
        "\n",
        "# Compare results.\n",
        "print(f'Backward method correct: {error(in_grad, correct_in_grad) < 1e-5}')"
      ],
      "id": "c94f0390",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f75a9567"
      },
      "source": [
        "### 2 Loss Function (20 Points)\n",
        "\n",
        "---\n",
        "\n",
        "For our model we want to use the cross-entropy loss that we already implemented in the previous problem set. However, this time we not only want to compute the gradient of the loss with respect to the input to the loss function, but with respect to all the parameters in our model. So we want to be able to call our loss function like this:\n",
        "\n",
        "```python\n",
        "# Outside training loop\n",
        "loss = CrossEntropyLoss(model)\n",
        "\n",
        "# Inside training loop\n",
        "outputs = model(inputs)\n",
        "\n",
        "current_loss = loss(outputs, labels)\n",
        "\n",
        "loss.backward()\n",
        "```\n",
        "\n",
        "After calling the `backward` method on the loss, the gradients with respect to all the weights and biases should be stored in the `grad` dictionary of the respective layer, so that we can call the optimizer to actually perform the updates. Calling the model and loss directly as in the example code above is just a shortcut for calling the respective `forward` methods, which we implemented for convenience.\n",
        "\n",
        "This is not a huge change compared to what we have already done, though.\n",
        "\n",
        "The only difference is that we split the computation of the loss and the computation of the gradient into two methods, and instead of just returning the gradient with respect to the function's input, we also pass it to the `backward` method of the model, which we stored in the constructor of the loss.\n",
        "\n",
        "\n",
        "### 2.1 Cross-entropy Loss (20 Points)\n",
        "\n",
        "---\n",
        "\n",
        "Now, let's recap the definition of the cross-entropy loss. For a single pair $(s^{(i)}, y_i)$ with input $s^{(i)}$ and target $y_i$, the cross-entropy loss is defined as\n",
        "\n",
        "$$\n",
        "    L(s^{(i)})\n",
        "    =\n",
        "    -\\ln\\frac{e^{s_{y_i}^{(i)}}}{\\sum_{j=1}^K e^{s_j^{(i)}}}\n",
        "    =\n",
        "    -s_{y_i}^{(i)} + \\ln\\sum_{j=1}^K e^{s_j^{(i)}},\n",
        "$$\n",
        "\n",
        "where $s^{(i)} \\in \\mathbb{R}^K$ is a vector of scores for $K$ classes, which will be the output of our last network layer. The total loss for a set $S = \\{s^{(i)}\\}_{i=1}^N$ of $N$ examples is just the average of the losses for a single input, that is\n",
        "\n",
        "$$\n",
        "    \\mathcal{L}(S) = \\frac{1}{N} \\sum_{i=1}^N L(s^{(i)}).\n",
        "$$\n",
        "\n",
        "The gradient of the loss $L$ for the $i$-th input with respect to the $k$-th entry is\n",
        "\n",
        "<br>\n",
        "\n",
        "$$\n",
        "    \\nabla L(s^{(i)})_k = \\frac{\\partial L(s^{(i)})}{\\partial s_k^{(i)}}\n",
        "    =\n",
        "    \\begin{cases}\n",
        "        \\sigma(s^{(i)})_k - 1 & \\text{if} \\enspace k = y_i \\\\[0.5em]\n",
        "        \\sigma(s^{(i)})_k & \\text{if} \\enspace k \\neq y_i,\n",
        "    \\end{cases}\n",
        "$$\n",
        "\n",
        "where $\\sigma$ denotes the softmax function, defined as\n",
        "\n",
        "$$\n",
        "    \\sigma(s)_k = \\frac{e^{s_k}}{\\sum_{j=1}^K e^{s_j}}.\n",
        "$$\n",
        "\n",
        "Regarding the gradient of the total loss $\\mathcal{L}$ with respect to the function inputs, we can use the linearity of the differential operator to obtain\n",
        "\n",
        "$$\n",
        "    \\nabla \\mathcal{L}(s^{(i)})_k = \\frac{1}{N} \\sum_{i=1}^N \\nabla L(s^{(i)})_k.\n",
        "$$"
      ],
      "id": "f75a9567"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec8c048a"
      },
      "source": [
        "##### Task\n",
        "\n",
        "Complete the definition of the `CrossEntropyLoss` class in the `loss.py` file. Your implementation should be fully vectorized, so no loops are allowed.\n",
        "\n",
        "In the `forward` method, compute the total loss for the given set of inputs. Since we need the output of the softmax function both for the computation of the loss and the computation of the gradient, you should save this intermediate result for later use in the backward pass. Store the loss in the `outputs` variable that is returned from the method.\n",
        "\n",
        "In the `backward` method, compute the gradient of the loss with respect to the input of the loss function and store the value in the `in_grad` variable. Then pass the gradient to the `backward` method of the model stored in the `self.model` attribute, to backpropagate the gradient through the entire network.\n",
        "\n",
        "##### Results\n",
        "\n",
        "To test your implementation, run the following code cells."
      ],
      "id": "ec8c048a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7895d07"
      },
      "source": [
        "# Create inputs.\n",
        "inputs = np.array([\n",
        "    [ 1.7, -2.5, -5.3, 4.1],\n",
        "    [-1.2,  1.8,  2.0, 2.5],\n",
        "    [ 2.3, -1.7, -0.4, 0.6]\n",
        "])\n",
        "\n",
        "# Create targets.\n",
        "labels = np.array([0, 3, 2])\n",
        "\n",
        "# Create dummy model.\n",
        "model = Model()\n",
        "\n",
        "# Create loss.\n",
        "loss = CrossEntropyLoss(model)"
      ],
      "id": "f7895d07",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34586663"
      },
      "source": [
        "You can test your `forward` method with the following code."
      ],
      "id": "34586663"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ae3706e"
      },
      "source": [
        "# Compute forward pass.\n",
        "test_loss = loss(inputs, labels)\n",
        "\n",
        "correct_loss = 2.060289248015619\n",
        "\n",
        "print(f'Forward method correct: {abs(test_loss - correct_loss) < 1e-5}')"
      ],
      "id": "6ae3706e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0097656d"
      },
      "source": [
        "You can test your `backward` method with the following code."
      ],
      "id": "0097656d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "696ab5b4"
      },
      "source": [
        "# Compute forward pass.\n",
        "loss(inputs, labels)\n",
        "\n",
        "# Compute backward pass.\n",
        "in_grad = loss.backward()\n",
        "\n",
        "correct_in_grad = np.array([\n",
        "    [-3.05645734e-01,  4.15191527e-04,  2.52478228e-05,  3.05205294e-01],\n",
        "    [ 3.87302498e-03,  7.77917862e-02,  9.50151022e-02, -1.76679913e-01],\n",
        "    [ 2.62838751e-01,  4.81405965e-03, -3.15669120e-01,  4.80163093e-02]\n",
        "])\n",
        "\n",
        "# Compare results.\n",
        "print(f'Backward method correct: {error(in_grad, correct_in_grad) < 1e-5}')"
      ],
      "id": "696ab5b4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acb0e277"
      },
      "source": [
        "### 3 Optimization (10 Points)\n",
        "\n",
        "---\n",
        "\n",
        "So far we have only used vanilla gradient descent. That is, the update rule was to scale the gradient with the learning rate and subtract the result from the parameters. In this exercise we want to extend that concept a bit, taking into account also the previous updates.\n",
        "\n",
        "### 3.1 SGD with Momentum (10 Points)\n",
        "\n",
        "---\n",
        "\n",
        "One of the problems with standard gradient descent is that the gradient with respect to a parameter may change rapidly during training. These oscillations of the gradient make optimization hard. In addition, there is also the problem of the gradient being stuck in a flat region, where the slope is almost zero. Gradient descent with momentum is one approach to tackle these problems.\n",
        "\n",
        "Instead of just updating the parameters with\n",
        "\n",
        "$$\n",
        "    W^{(t)} = W^{(t-1)} - \\lambda\\nabla\\mathcal{L}(W^{(t-1)})\n",
        "$$\n",
        "\n",
        "where $\\lambda$ is the learning rate, we also take into account the previous updates of the parameters, scaled by a hyperparameter called momentum. Thus the update rule becomes\n",
        "\n",
        "$$\n",
        "    W^{(t)} = W^{(t-1)} - V^{(t)}\n",
        "$$\n",
        "\n",
        "with\n",
        "\n",
        "$$\n",
        "    V^{(t)} = \\lambda\\nabla\\mathcal{L}(W^{(t-1)}) + \\mu V^{(t-1)}\n",
        "$$\n",
        "\n",
        "where $V$ is called velocity and $\\mu$ is the momentum. The velocity is an array with the same shape as $W$ and $\\nabla\\mathcal{L}(W)$ and can be understood as a moving average over the past gradients. With this update rule we get a more stable trajectory towards a minimum and we can still move, even if the gradient for the current time step becomes small."
      ],
      "id": "acb0e277"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e69ee5d"
      },
      "source": [
        "##### Task\n",
        "\n",
        "Complete the definition of the `SGD` class in the `optim.py` file. Your implementation should be fully vectorized, so no loops are allowed.\n",
        "\n",
        "In the `step` method of the optimizer, implement the update rule described above. The $\\text{learning_rate}$ and $\\text{momentum}$ are stored as attributes of the optimizer object and each layer that is iterated over has dictionaries for parameters, gradients and velocity, where corresponding entries are referenced with the same name, given that you adhered to this convention in the previous exercises.\n",
        "\n",
        "##### Results\n",
        "\n",
        "To test your implementation, run the following code cell."
      ],
      "id": "5e69ee5d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6f8877bf"
      },
      "source": [
        "class Net(Model):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.fc = Linear(3, 2, bias=False)\n",
        "\n",
        "# Create dummy model.\n",
        "model = Net()\n",
        "\n",
        "# Set shape for weights, gradient and velocity.\n",
        "shape = (3, 2)\n",
        "\n",
        "# Create weights.\n",
        "weights = np.linspace(-0.3, 0.5, num=np.prod(shape))\n",
        "weights = np.reshape(weights, shape)\n",
        "\n",
        "# Add weights to layer.\n",
        "model.fc.param['weights'] = weights\n",
        "\n",
        "# Create gradient.\n",
        "grad = np.linspace(-0.2, 0.3, num=np.prod(shape))\n",
        "grad = np.reshape(grad, shape)\n",
        "\n",
        "# Add gradient to layer.\n",
        "model.fc.grad['weights'] = grad\n",
        "\n",
        "# Create optimizer.\n",
        "optimizer = SGD(model, learning_rate=0.1, momentum=0.5)\n",
        "\n",
        "# Create velocity.\n",
        "velocity = np.linspace(-0.1, 0.4, num=np.prod(shape))\n",
        "velocity = np.reshape(velocity, shape)\n",
        "\n",
        "# Add velocity to layer.\n",
        "model.fc.velocity['weights'] = velocity\n",
        "\n",
        "# Compute update step.\n",
        "optimizer.step()\n",
        "\n",
        "correct_weights = np.array([\n",
        "    [-0.23, -0.13],\n",
        "    [-0.03,  0.07],\n",
        "    [ 0.17,  0.27]\n",
        "])\n",
        "\n",
        "# Compare results.\n",
        "print(f'Step method correct: {error(weights, correct_weights) < 1e-5}')"
      ],
      "id": "6f8877bf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "690d5521"
      },
      "source": [
        "### 4 Deep Neural Network (10 Points)\n",
        "\n",
        "---\n",
        "\n",
        "Now that we have all the components implemented, we can plug everything together to create a deep neural network.\n",
        "\n",
        "We want to train our model again on the CIFAR-10 dataset that we already used in the previous problem sets. The function for loading and preprocessing the data expects the dataset in the `datasets` folder in the same directory as the notebook, so copy the folder before you proceed."
      ],
      "id": "690d5521"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4057f215"
      },
      "source": [
        "# Load and preprocess the CIFAR-10 dataset.\n",
        "data = get_CIFAR_10_data()\n",
        "\n",
        "# Output the shapes of the partitioned data and labels.\n",
        "for name, array in data.items():\n",
        "    print(f'{name} shape: {array.shape}')"
      ],
      "id": "4057f215",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd19fc3e"
      },
      "source": [
        "Since we don't have GPU support, we're going to create a rather shallow model. Otherwise the training would take too much time.\n",
        "\n",
        "To test our implementations, we're going to define a network with two blocks of convolutional layers, each followed by a ReLU activation function and a max pooling layer. We convert the outputs of the last pooling layer into vectors and pass them into a small fully-connected network, composed of two linear layers, the first of which has a ReLU activation function, followed by a dropout layer. The last linear layer has no activation function and produces the scores for the ten classes of the dataset.\n",
        "\n",
        "Let's create this model using the `Model` base class."
      ],
      "id": "fd19fc3e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7edaa5b4"
      },
      "source": [
        "class Net(Model):\n",
        "    \n",
        "    def __init__(self):\n",
        "\n",
        "        # Convolution with ReLU and max pool\n",
        "        self.conv1 = Conv2D(3, 6, kernel_size=3, padding=1)\n",
        "        self.relu1 = ReLU()\n",
        "        self.pool1 = MaxPool(2)\n",
        "\n",
        "        # Convolution with ReLU and max pool\n",
        "        self.conv2 = Conv2D(6, 8, kernel_size=3, padding=1)\n",
        "        self.relu2 = ReLU()\n",
        "        self.pool2 = MaxPool(2)\n",
        "\n",
        "        # Tensors to vector conversion\n",
        "        self.vec = Vector()\n",
        "\n",
        "        # Fully connected with ReLU\n",
        "        self.fc1 = Linear(512, 32)\n",
        "        self.relu3 = ReLU()\n",
        "\n",
        "        # Dropout\n",
        "        self.drop = Dropout(0.75)\n",
        "\n",
        "        # Fully connected\n",
        "        self.fc2 = Linear(32, 10)"
      ],
      "id": "7edaa5b4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c7cfddd"
      },
      "source": [
        "### 4.1 Training (10 Points)\n",
        "\n",
        "---\n",
        "\n",
        "To train the model, we make use of the `Solver` class defined in the `solver.py` file.\n",
        "\n",
        "##### Task\n",
        "\n",
        "Train the model you implemented for at least $1$ epoch on the complete training set and show the results. If you use the default values of the solver, you should see a training and validation accuracy of at least $\\sim 30 \\%$ after one epoch. There should be no overfitting up to this point. In order to first check if your model converges, you can use the development set (`X_dev`, `y_dev`) from the `data` dictionary instead of the whole training set.\n",
        "\n",
        "##### Results\n",
        "\n",
        "You can use the `verbose` and `print_every` parameters of the solver to print out results during training."
      ],
      "id": "8c7cfddd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a1edcea"
      },
      "source": [
        "# Create the model.\n",
        "model = Net()\n",
        "\n",
        "# Define the training and validation set.\n",
        "training_data = {\n",
        "    'X_train': data['X_train'],\n",
        "    'y_train': data['y_train'],\n",
        "    'X_val': data['X_val'],\n",
        "    'y_val': data['y_val']\n",
        "}\n",
        "\n",
        "# Create the solver.\n",
        "solver = Solver(model, training_data, num_epochs=1, verbose=True)\n",
        "\n",
        "# Train the model.\n",
        "results = solver.train()"
      ],
      "id": "8a1edcea",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4b1af15"
      },
      "source": [
        ""
      ],
      "id": "c4b1af15",
      "execution_count": null,
      "outputs": []
    }
  ]
}